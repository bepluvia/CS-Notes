# CS231n学习手记

## 问题记录

## P5

1. L1距离与L2距离区别，什么时候用哪个？（重听课程并总结）

+ L1距离取决于坐标系统。如果转动坐标轴，将会改变点之间的L1距离。

  > 如在$e1 = (1, 0), e2 = (0, 1)$坐标系下，$a = (1, 1)$与$b = (1, 2)$之间的L1距离为1。
  >
  > 而在$e1' = (\frac{\sqrt{2}}{2}, \frac{\sqrt{2}}{2}), e2' = (\frac{\sqrt{2}}{2}, -\frac{\sqrt{2}}{2})$坐标系下（逆时针旋转45°），$a' = (\sqrt{2}, 0)$与$ b' = (\frac{3\sqrt{2}}{2}, \frac{\sqrt{2}}{2})$之间的L1距离为$\sqrt{2}$。
  >
  > （随后可补充图片）

+ L2不受坐标系变化的影响（描述两点的直线距离）。

+ 对于选择哪一种：

  1. 如果输入的特征向量中的某些值对于任务有**重要意义**，使用L1距离。

  > 比如对公司某员工进行考察，员工向量的不同元素有固定**实际含义**，如薪资、工作年限等，L1可能会更合适。

  2. 如果只是某个空间中的一个通用向量，不知道其中不同元素的实际含义，L2距离更加自然。
  3. 但对于这种**超参数**，建议都尝试下，看哪个效果更好。

## P7

### 1. 为什么scores[j] - correct_class_score + 1要加1？

用来确保correct_class_score在一定程度上大于其他分数（scores[j]），否则如果两个分数相近也没有loss来更新参数，这样**可以“激励”往更好的方向更新参数**。

> 我们并不关心损失函数中分数的绝对值，我们只关心不同分数（如这里的scores[j]与correct_class_score ）的相对差值，我们只需要正确分类的分数要远远大于不正确分类的分数。（如果将整个W的参数放大或缩小，所有的得分也将会缩放，此时这个free parameter（最后加的1）的重要性也会随之变化，可能就得相应的取5或者0.2，主要是看两个分数的差值是否是我们想要的）
>
> 关于这个细节的详细解释，可以深入查看课程讲义。

### 2. 不同损失函数有什么不同？

如果我们将原来差值之和变为差值的平方和，与原来有什么不同？

<img src="./assets/image-20240904215816670.png" alt="image-20240904215816670" style="zoom: 50%;" />

损失函数是用来衡量分类器的效果好坏，

**平方和损失函数：**对于一个很坏的分类器，会将其loss翻倍，很大程度加深优化器对这个分类器的判断，从而使其在后续过程中能够迅速优化。

**合页损失函数（hinge loss funtion）：**对微小的错误并不在意，如果分类出现很多错误，那么loss会相应加深。（？没听懂）

总之，不容损失函数都有其用武之地，使用时需要选择合适的。

### 3. 矢量化处理技巧

先进行全部的矢量计算，然后再将需要为0的手动清零

<img src="./assets/image-20240904215837942.png" alt="image-20240904215837942" style="zoom:50%;" />

### 4. 正则项

在训练过程中，我们并不关心训练集的表现，而是关心分类器的测试数据。

过拟合会导致模型过于复杂，在测试集的表现不好，因此引入正则项，适当降低模型复杂度。（奥卡姆剃刀原理）

<img src="./assets/image-20240904215851196.png" alt="image-20240904215851196" style="zoom:50%;" />

比较常用的正则项：

<img src="./assets/image-20240904215903279.png" alt="image-20240904215903279" style="zoom:50%;" />

L2正则偏向每个参数对模型都起一定作用，maybe更鲁棒。

而L1正则会偏向更稀疏的W，即W中的0越多，maybe L1正则项会更小一些，相应的loss也会更低。

### 5. Gradient 梯度

梯度是一个偏导数的向量，它指向了函数增加最快的方向。相应的负梯度就指向了函数下降最快的方向（在优化中表现为loss下降最快的方向）。

> 一个”地形“任意方向的斜率 = 该点的梯度和该点单位方向向量的点积。

数值差分法：依次将W的一个参数增加一个很小的数值（0.00001），保持其他不变重新计算loss，用Δloss / 0.00001就可以得到该参数的梯度。

<img src="./assets/image-20240904215916261.png" alt="image-20240904215916261" style="zoom:50%;" />

这样做很耗时，但是在测试时有时还是需要数值计算：

![image-20240904215925967](./assets/image-20240904215925967.png)

> 确定学习率（步长）是第一个需要确定的超参数，很重要。



### P13 不同激活函数的优缺点

#### sigmoid函数（缺点）

sigmoid函数的其中一个缺点是“sigmoid函数的输出不是以0”为中心的：

<img src="./assets/image-20240904220044475.png" alt="image-20240904220044475" style="zoom:50%;" />

这样有什么坏处？

视频中提到，对于一个函数如$f(x) = W * X + b$，这样我们计算W梯度$\frac{df}{dw} = X$可以看到W的梯度就是上游梯度dot X，这样会有什么隐患？

那就是如果**X是一个全为正数的矩阵**当我们**更新W时，只能朝右上或者左下方向更新**（比如W有两个维度w1和w2，W只能落在一三象限中），如果合理的W梯度方向为右下，则更新起来效率会很低。

<img src="./assets/image-20240904220455160.png" alt="image-20240904220455160" style="zoom: 50%;" />

但是这样看来，梯度只与X有关系，与sigmoid有什么关系呢？

当然有关系，因为后面一层会接受前一层的“输出—激活”，**如果是sigmoid函数，那激活后的X将全部为正值**，也就会出现上述情况。

**一个好的X尽量要让均值为0，这样梯度更新时才能很好地优化。（也就是说激活函数最好是zero-centered）**





### 作业

**assignment 1 ——svm.ipynb**

对于svm的grad计算：

> grad如何计算：**要列公式再看！！！**
>
> 1. 首先看有几个参数对Li产生影响，这里就是有两个$w_j$和$w_{yi}$
> 2. 分别用这些参数对Li求导
> 3. 在这一次loss计算中，要同时更新这两个参数的梯度

![image-20240831154825881](./assets/image-20240831154825881.png)





