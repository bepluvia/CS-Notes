# Baseline



# 项目过程



## 召回阶段

> 采用多路召回，融合以下几种方式：
>
> 1. 基于itemcf计算的item之间的相似度sim进行的召回 
> 2. 基于embedding搜索得到的item之间的相似度进行的召回
> 3. YoutubeDNN召回
> 4. YoutubeDNN得到的user之间的相似度进行的召回
> 5. 基于冷启动策略的召回

### 1. itemcf 物品协同过滤

> 核心思想：利用文章与文章之间的相似性矩阵进行推荐。

#### 1.1 构建itemcf_i2i_sim矩阵

构建itemcf_i2i_sim矩阵的过程：

1. 先获取“根据点击时间获取用户的点击文章序列”

> get_user_item_time：{user1: [(item1, time1), (item2, time2), ...], ...}

2. 三重循环构建物品相似度矩：（矩阵大小为 i2i_sim.shape = [item_size, item_size]）

   （1）第一重循环，先遍历dict，取得不同user对物品点击序列：[(item1, time1), (item2, time2), ...]；

   （2）第二重遍历这个物品序列，取得“行方向的物品item_i”；

   （3）第三重循环遍历物品序列的“其他物品item_j”（碰到item_i就continue），然后为i2i_sim\[i][j]累加相似度得分，每次累加1 / math.log(len(item_time_list) + 1)；（最普通的itemcf做法）

   > 加权itemcf：（加入位置权重、时间权重、时间间隔信息）

   ```python
   # 考虑文章的正向顺序点击和反向顺序点击    
   loc_alpha = 1.0 if loc2 > loc1 else 0.7
   # 位置信息权重，其中的参数可以调节
   loc_weight = loc_alpha * (0.9 ** (np.abs(loc2 - loc1) - 1))
   # 点击时间权重，其中的参数可以调节
   click_time_weight = np.exp(0.7 ** np.abs(i_click_time - j_click_time))
   # 两篇文章创建时间的权重，其中的参数可以调节
   created_time_weight = np.exp(0.8 ** np.abs(item_created_time_dict[i] - item_created_time_dict[j]))
   i2i_sim[i].setdefault(j, 0)
   # 考虑多种因素的权重计算最终的文章之间的相似度
   i2i_sim[i][j] += loc_weight * click_time_weight * created_time_weight / math.log(len(item_time_list) + 1)
   ```

3. 将相似度得分转换为余弦相似度$\text{sim}_{uv} = \frac{|\mathcal{N}(u) \cap \mathcal{N}(v)|}{\sqrt{|\mathcal{N}(u)| \cdot |\mathcal{N}(v)|}}$，由于2. 过程中可以记录每个物品出现次数，因此这里直接使用双重循环依次除公式分母：

   ```python
   i2i_sim_ = i2i_sim.copy()
   for i, related_items in i2i_sim.items():
       for j, wij in related_items.items():
           i2i_sim_[i][j] = wij / math.sqrt(item_cnt[i] * item_cnt[j])
   ```

#### 1.2 利用itemcf_i2i_sim矩阵做协同过滤

1. 先获取“根据点击时间获取用户的点击文章序列”（函数get_user_item_time）

2. 三重循环，获得物品的得分：

   （1）第一重循环，遍历user_id，转入函数item_based_recommend继续后面的循环；

   （2）第二重循环遍历该用户交互过的文章：[(item1, time1), (item2, time2), ...]；

   （3）第三重循环，先根据（2）中的“交互过的文章”查看与该item最相近的sim_item_topk篇文章，获取文章id以及sim权重，将sim权重相加便得到了该文章的rank得分（最基本）（直观理解就是，文章与“交互过的文章”相似度越高，则更偏向给用户推荐此文章）；

   > 这里在此基础上做了改进：
   >
   > - 加入时间权重（时间间隔越长，sim占比越小）
   > - 位置权重（位置相隔越远，sim占比越小）
   > - 内容权重（使用emb_i2i_sim衡量，emb_i2i_sim\[i][j]和emb_i2i_sim\[j][i]与content_weight之和得到，值越大则sim占比越大）

   3. 如果只靠sim召回的物品数（sim_item_topk）不足recall_item_num，则用点击数量最多的文章填充（item_topk_click，即热门商品补全）
   4. 最终返回格式：{user_id: {item_id: score, ...}, ...}

### 2. Usercf 用户协同过滤

#### 2.1 构建usercf_u2u_sim矩阵

1. 先获得用户活跃度dict，备用：统计每个用户点击次数，并归一化到[0, 1]之间；

2. 获取“根据点击时间获取文章被点击的用户序列”（函数get_item_user_time_dict）

3. 三重循环构建物品相似度矩：（矩阵大小为 u2u_sim.shape = [user_size, user_size]）

   （1）第一重循环，先遍历item_user_time_dict，取得不同item被点击的用户序列：[(user1, time1), (user2, time2), ...]；

   （2）第二重遍历这个用户序列，取得“行方向的用户user_i”；

   （3）第三重循环遍历用户序列的“其他用户user_j”（碰到user_i就continue），然后为u2u_sim\[i][j]累加相似度得分，每次累加1 / math.log(len(item_time_list) + 1)；（最普通的usercf做法）

   > 这里引入1. 获得的用户活跃度，user_i和user_j的活跃度越高，则相似度越大：
   >
   > activate_weight = 100 * 0.5 * (user_activate_degree_dict[u] + user_activate_degree_dict[v])   
   >
   > u2u_sim\[u][v] += activate_weight / math.log(len(user_time_list) + 1)

4. 将相似度得分转换为余弦相似度。

#### 2.2 利用usercf_u2u_sim矩阵做协同过滤

1. 先获取“根据点击时间获取文章被点击的用户序列”（函数get_item_user_time_dict）

2. 获取所有用户的交互序列user_item_time_dict：{user1: [(item1, time1), (item2, time2)..]...}

3. 经过四重循环，才获得协同过滤的得分：

   （1）第一重循环，遍历user_id，转入函数user_based_recommend继续后面的循环；

   （2）第二重循环遍历的是，u2u_sim[user_id].items()先根据相似度分数倒排，然后取前sim_user_topk的结果：[(user_1, score_1), (user_2, score_2), ...]；

   （3）通过user_item_time_dict查看u2u_sim中“其他user”的交互序列，并遍历这个交互序列取得需要rank的item_id；

   （4）items_rank[item_id(3)] += user_sim_score（这里就是（2）中的用户分数），这就是最基础的usercf（直观理解就是，取与当前user最相似的user，用他们看过的item给所有item打分，他们给的评分越高，更偏向于给用户推荐这个文章）；

   > 这里也引入了：点击时的相对位置权重、内容相似性权重、创建时间差权重，与itemcf类似。

4. 最后如果召回不够，也要进行热度补全；



### 3. item embedding sim

> 使用Embedding计算item之间的相似度是为了后续冷启动的时候可以获取未出现在点击数据中的文章

#### 3.1 构建emb_i2i_sim相似度矩阵

1. 取出emb_df中的value部分，并归一化；

2. 建立faiss索引，为每个item查询topk个相似item，并返回相似度sim和索引idx两个矩阵：

   sim = [[1.00, 0.95, ...],  # 101 和自己、102、... 的相似度
               [1.00, 0.93, ...],  # 102 和自己、101、... 的相似度
               [1.00, 0.85, ...],  # 103 和自己、101、... 的相似度
               ...]
   idx = [[0, 1, ...],  # 对应sim中每一行中，每个数对应的index(item_idx_2_rawid_dict的key)
              [1, 0, ...],
              [2, 0, ...],
              ...]

3. 双重循环得到item_sim_dict：（形如：{article_id:{other_article_id: sim, ...}, ...}）

   （1）一重循环遍历zip(range(len(item_emb_np)), sim, idx)，得到物品id、该物品与其他物品的相似度list、这些“其他物品”的id list；

   （2）第二重循环遍历zip(rele_idx_list[1:], sim_value_list[1:])，得到具体的“其他物品的相似度”以及该物品的id；

   （3）为相似度item_sim_dict\[target_raw_id][rele_raw_id]累加两个物品相似度即可；（由于faiss计算得到的就是余弦相似度，因此不用再除模）

#### 3.2 利用emb_i2i_sim矩阵做协同过滤

过程与itemcf_i2i_sim一样，只不过这里用的是emb_i2i_sim；



### 4. youtubeDNN召回

#### 4.1 构建数据集

1. 将点击数据按点击时间，升序排列；

2. 遍历data.groupby('user_id')，得到user_id，和这个user的交互序列；

3. 使用滑动窗口构建正负样本（增加样本量）：

   > 滑动窗口：假设user_1的交互序列为[(item_1. Time_1), ...]（实际交互了10个item），那么将依次构建交互长度为1、2、3 ... 10的交互序列样本，依次可以由一个用户的交互，构建出多个训练数据；

   测试数据使用滑动窗口中最长的窗口，即实际交互序列，目的是“更完整地还原用户的兴趣信息，从而更准确地评估召回效果”。



#### 4.2 训练模型并获取user/item_emb

训练YoutubeDNN，然后单独分离出user/item_emb推理子网络，利用子网络推理得到user/item_emb：

```python
# 单独分离出user_emb推理子网络
user_embedding_model = Model(inputs=model.user_input, outputs=model.user_embedding)
# 单独分离出item_emb推理子网络
item_embedding_model = Model(inputs=model.item_input, outputs=model.item_embedding)  

# 保存当前的item_embedding 和 user_embedding 排序的时候可能能够用到，
# 但是需要注意保存的时候需要和原始的id对应
user_embs = user_embedding_model.predict(test_user_model_input, batch_size=2 ** 12)
item_embs = item_embedding_model.predict(all_item_model_input, batch_size=2 ** 12)
```



#### 4.3 利用user/item_emb构建youtube_u2i_dict

1. 建立faiss索引，对user_emb和item_emb做向量内积求余弦相似度，获得每个user最相似的topk个item，返回得到sim（sim\[i][j]表示第 i 个用户与第 j 个 top 相似 item 的 相似度（内积））和idx（idx\[i][j]表示第 i 个用户召回的第 j 个 item 在 item_embs 中的索引）；

2. 双重遍历得到user_recall_items_dict：

   （1）遍历zip(test_user_model_input['user_id'], sim, idx)，得到user_id，user与item对相似度list，item_id list；

   （2）遍历zip(rele_idx_list[1:], sim_value_list[1:])，得到item相似度和item_id，然后将其累加到user_recall_items_dict\[target_raw_id][rele_raw_id]得到u2i的召回评分

#### 4.4 user embedding sim召回

youtubeDNN自网络得到的user_emb可以得到youtube_u2u_sim，然后可以做基于user_emb的usercf召回；



### 5. 召回冷启动

还是使用item_based_recommend，只不过这里召回的item要比较多（正常可能是10～30，这里召回150+），然后筛选这些冷启动文章，只有通过筛选的最终才能加入冷启动序列中；

> 这里的思想是：查看用户浏览过的item，推荐与这些item相似的文章，但是一般冷启动是因为用户没有浏览过文章，这时利用item_based_recommend的机制，就是如果依靠相似度召回得到的文章不够多，可以用“热门商品补全”，就弥补了这个问题。



### 多路召回融合

1. 基于itemcf计算的item之间的相似度sim进行的召回 
2. 基于embedding搜索得到的item之间的相似度进行的召回
3. YoutubeDNN召回
4. YoutubeDNN得到的user之间的相似度进行的召回
5. 基于冷启动策略的召回

给每种召回方式一个权重：

```python
weight_dict = {'itemcf_sim_itemcf_recall': 1.0,
               'embedding_sim_item_recall': 1.0,
               'youtubednn_recall': 1.0,
               'youtubednn_usercf_recall': 1.0, 
               'cold_start_recall': 1.0}
```

最终把不同召回得分，进行加权求和，然后按最终得分排序。

## 特征工程

### 1. 数据读取

#### 1.1 数据集划分

1. 读取 训练集用户点击日志文件 train_click_log.csv；

2. 随机抽取sample_user_nums个index作为验证集val，其余作为训练集trn；

3. 验证集：按['user_id', 'click_timestamp']升序排列，**取每个user的最后一次点击作为答案**，然后将最后一行在原本val数据中除去：

   ```python
   click_val = click_val.sort_values(['user_id', 'click_timestamp'])
   val_ans = click_val.groupby('user_id').tail(1)
   
   click_val = click_val.groupby('user_id').apply(lambda x: x[:-1]).reset_index(drop=True)
   ```

   还需要去除val_ans中用户只有一个点击数据的情况，因为如果该用户只有一个点击数据，又被分到ans中；

   > 这样就会出现**没有用户历史行为序列**的问题，可能导致报错或评估不准确。

   ```python
   val_ans = val_ans[val_ans.user_id.isin(click_val.user_id.unique())] # 只选click_val中出现的user_id的数据
   click_val = click_val[click_val.user_id.isin(val_ans.user_id.unique())]  # 只选val_ans中出现的user_id的数据
   ```

4. 训练集：按['user_id', 'click_timestamp']升序排列，**取每个user的最后一次点击作为答案**，然后将最后一行在原本val数据中除去，这里**稍有不同**如果用户只有一次点击，则会在历史序列中保留这条数据：
   ```python
   # 获取当前数据的历史点击和最后一次点击
   def get_hist_and_last_click(all_click):
       all_click = all_click.sort_values(by=['user_id', 'click_timestamp'])
       click_last_df = all_click.groupby('user_id').tail(1)
   
       # 如果用户只有一个点击，hist为空了，会导致训练的时候这个用户不可见，此时默认泄露一下
       def hist_func(user_df):
           if len(user_df) == 1:
               return user_df
           else:
               return user_df[:-1]
   
       click_hist_df = all_click.groupby('user_id').apply(hist_func).reset_index(drop=True)
   
       return click_hist_df, click_last_df
   ```

   #### 1.2 对训练数据做负采样

   > 对应函数：get_user_recall_item_label_df

   1. 读取召回队列；

      ```python
      # 读取召回列表
      recall_list_dict = get_recall_list(save_path, single_recall_model='i2i_itemcf') # 这里只选择了单路召回的结果，也可以选择多路召回结果
      # 将召回数据转换成df
      recall_list_df = recall_dict_2_df(recall_list_dict)
      ```

   2. 为召回的队列打标签label，这样就可以对召回队列做监督学习，具体过程如下：

      > 将召回队列df与用户最后一次点击队列df(click_trn_last)，按照['user_id', 'sim_item']合并；
      >
      > 用户最后一次点击队列df中有feature：['click_timestamp']，合并之后，若召回队列中不是用户最后一次点击的会默认填充为NaN，若用户有最后一次点击则会是click_trn_last的click_timestamp值；
      >
      > 所以可以设置label为，若click_timestamp不为nan，则label设1，反之设0；

   3. 对**训练集**中的负样本按user_id/item_id分组后分别进行采样，使得负样本不多不少（合理控制召回数据中的负样本数量和分布）；

   4. 如果有**验证集**，同样需要对验证集的负样本进行采样；

   5. **测试集**无需进行负采样，直接将其所有数据的label设置为-1即可，表示这是测试集数据；

### 2. 特征工程

#### 2.1 制作与用户历史行为相关特征

补充用户行为历史特征，其中包括：

```python
id_cols = ['user_id', 'click_article_id']
sim_cols = ['sim' + str(i) for i in range(N)]
time_cols = ['time_diff' + str(i) for i in range(N)]
word_cols = ['word_diff' + str(i) for i in range(N)]
sat_cols = ['sim_max', 'sim_min', 'sim_sum', 'sim_mean']
user_item_sim_cols = ['user_item_sim'] if user_emb else []
user_score_rank_label = ['score', 'rank', 'label']
```
具体含义如下所示：

| 特征名                             | 说明                                                     |
| ---------------------------------- | -------------------------------------------------------- |
| `sim0 ~ simN-1`                    | 当前候选文章与用户最近 N 篇点击文章的相似度（余弦/点积） |
| `time_diff0 ~ time_diffN-1`        | 候选文章发布时间与历史文章的时间差                       |
| `word_diff0 ~ word_diffN-1`        | 候选文章字数与历史文章的字数差                           |
| `sim_max/sim_min/sim_sum/sim_mean` | 相似度的统计特征                                         |
| `user_item_sim`                    | 候选文章与用户整体向量的相似度（如果传入用户向量）       |
| `score`                            | 召回时计算的得分（如 i2i 得分）                          |
| `rank`                             | 在该用户候选列表中的排名                                 |
| `label`                            | 是否点击（训练用）                                       |

#### 2.2 构造用户和文章相关的特征

现将train、test、val点击数据进行合并，然后拼接上文章信息df（这里统一做处理，处理完之后再分开），得到all_data，后面都是基于all_data进行处理的。

##### 用户活跃度特征 user_act_fea

1. 取all_data的['user_id', 'click_article_id', 'click_timestamp']特征列；

2. 首先根据用户user_id分组， 对于每个用户，计算点击文章的次数， 两两点击文章时间间隔的均值；

3. 把点击次数取倒数和时间间隔的均值统一归一化，然后两者相加合并，该值越小， 说明用户越活跃；

   > 即，用户点击次数越多，点击时间间隔越小，表明用户越活越；

4. 注意， 上面两两点击文章的时间间隔均值， 会出现如果用户只点击了一次的情况，这时候时间间隔均值那里会出现空值， 对于这种情况最后特征那里给个大数进行区分；

这个的衡量标准就是先把点击的次数取倒数然后归一化， 然后点击的时间差归一化， 然后两者相加进行合并， 该值越小， 说明被点击的次数越多， 且间隔时间短。

##### 文章热度特征 article_hot_fea

1. 取all_data的['user_id', 'click_article_id', 'click_timestamp']特征列；

2. 根据文章进行分组， 对于每篇文章的用户， 计算点击的时间间隔；

3. 将用户的数量取倒数， 然后用户的数量和时间间隔归一化， 然后相加得到热度特征， 该值越小， 说明被点击的次数越大且时间间隔越短， 文章比较热；

   > 即，文章被点击次数越多，被点击间隔越小，说明文章热度越高；

##### 用户的习惯特征

这里就不详细说明怎么计算的，只写主要思想：

- 用户的设备习惯，取最常用的设备，具体做法：直接取用户使用设备的众数；

- 用户的时间习惯： 根据其点击过得历史文章的时间来做一个统计（这个感觉最好是把时间戳里的时间特征的h特征提出来，看看用户习惯一天的啥时候点击文章）， 但这里先用转换的时间吧， 求个均值，具体说法：对用户点击文章的时间戳转换为每天的时间，然后求均值；

- 用户的爱好特征， 对于用户点击的历史文章主题进行用户的爱好判别， 更偏向于哪几个主题，具体做法：可以用multi-hot进行编码；

- 用户文章的字数差特征， 用户的爱好文章的字数习惯，具体做法：对用户点击的所有文章的字数，求均值；

#### 2.3 特征工程处理后的全部特征

| 特征名                | 含义说明                                                     |
| --------------------- | ------------------------------------------------------------ |
| `user_id`             | 用户唯一标识                                                 |
| `click_article_id`    | 候选文章 ID，即推荐目标                                      |
| `sim0`                | 候选文章与用户最近点击文章的相似度（一般是 dot 或 cosine）   |
| `time_diff0`          | 候选文章与用户最近点击文章的发布时间差（如秒数）             |
| `word_diff0`          | 候选文章与用户最近点击文章的字数差                           |
| `sim_max`             | 候选文章与最近 N 篇历史文章相似度的最大值                    |
| `sim_min`             | 候选文章与最近 N 篇历史文章相似度的最小值                    |
| `sim_sum`             | 候选文章与最近 N 篇历史文章相似度的总和                      |
| `sim_mean`            | 候选文章与最近 N 篇历史文章相似度的平均值                    |
| `score`               | 召回阶段给候选文章的得分（如 i2i CF 得分）                   |
| `rank`                | 该候选文章在召回列表中的排名（得分从高到低）                 |
| `label`               | 是否点击了该候选文章（1 为正样本，0 为负样本，-1为测试数据） |
| `click_size`          | 用户历史点击的文章数量，表示用户活跃度的一个指标             |
| `time_diff_mean`      | 用户历史点击的时间间隔的平均值，衡量用户访问频率             |
| `active_level`        | 用户活跃等级                                                 |
| `click_environment`   | 用户点击时所处的环境（如 Web、App、小程序，类别变量）        |
| `click_deviceGroup`   | 用户设备分组（如 Android、iOS、PC，类别变量）                |
| `click_os`            | 用户操作系统（如 Android、iOS、Windows，类别变量）           |
| `click_country`       | 用户点击时的国家信息                                         |
| `click_region`        | 用户点击时的省份或区域信息                                   |
| `click_referrer_type` | 用户点击来源渠道（如搜索、推荐、广告等）                     |
| `user_time_hob1`      | 用户的时间偏好类型 1（用户点击时间偏好）                     |
| `user_time_hob2`      | 用户的时间偏好类型 2（文章建立时间偏好）                     |
| `cate_list`           | 用户主题偏好特征                                             |
| `words_hbo`           | 用户字数偏好特征                                             |

## 排序模型+模型融合

### 1. 排序模型

LGB排序模型（ndcg@1～5）+ LGB分类模型（auc）+ DIN（auc）

其中，**LGB排序模型结果：**

valid_0's ndcg@1: 0.946675	

valid_0's ndcg@2: 0.97778	

valid_0's ndcg@3: 0.97938	

valid_0's ndcg@4: 0.979703	

valid_0's ndcg@5: 0.979703

**LGB分类模型结果：**

valid_0's auc: 0.819127	

valid_0's binary_logloss: 0.446397

**DIN结果：**

loss: 0.2725 - binary_crossentropy: 0.2719 - auc: 0.9132 

val_loss: 0.2751 - val_binary_crossentropy: 0.2745 - val_auc: 0.9151

#### 2. 模型融合

方法一：直接对score做加权融合，取最高score的文章推荐；

方法二：额外训练一个聚合模型，给出预测分数和对应分数下的排名，然后做二值预测：

```python
feat_cols = ['pred_score_0', 'pred_rank_0', 'pred_score_1', 'pred_rank_1', 'pred_score_2', 'pred_rank_2']

trn_x = finall_trn_ranker_feats[feat_cols]
trn_y = finall_trn_ranker_feats['label']

# 定义模型
lr = LogisticRegression()

# 模型训练
lr.fit(trn_x, trn_y)

# 模型预测（lr.predict_proba(tst_x)[:, 1] 返回逻辑回归模型对测试集 tst_x 中每个样本属于「正类（label = 1）」的预测概率分数）
"""
lr.predict_proba(tst_x)
→ array([[0.8, 0.2],     # 第1个样本：不点概率 0.8，点击概率 0.2
         [0.3, 0.7],     # 第2个样本
         [0.5, 0.5]])    # 第3个样本
"""
finall_tst_ranker_feats['pred_score'] = lr.predict_proba(tst_x)[:, 1]
```

