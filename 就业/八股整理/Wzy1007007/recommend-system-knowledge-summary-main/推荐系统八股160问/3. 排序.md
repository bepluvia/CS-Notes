# 排序

### 推荐系统公开课第三章：排序+推荐算法实战第二章：特征工程

**1、多目标排序模型可以有哪些指标？最终排序的依据？针对的排序类型？**

指标：点击率、点赞率、收藏率、转发率等，最终是将每个指标得到的分数加权融合得到总分，对其进行排序，针对精排模型。

**2、简要说明多目标排序模型的结构？和双塔模型有什么不同？**

结构：将用户特征、物品特征、统计特征、场景特征串接融合进入神经网络，通过处理得到融合的特征向量，再通过全连接层+非线性输出指标分数（全连接层对每个指标而言是独立的）。

不同：双塔模型是用户向量和物品向量先各自进行神经网络处理，再将输出进行相似度求解操作，神经网络是独立的；而多目标排序是一开始将用户和物品向量进行融合，神经网络是共享的。

**3、排序模型的训练采用什么损失函数？训练难点在哪里？**

采用交叉熵损失函数，难点在于正负样本的类别数量极不平衡，需要对负样本降采样，由于负样本变少，预估点击率将大于真实点击率。

**4、MMoE的作用？模型结构？一般专家神经网络数量为多少？**

作用：采用若干个专家神经网络，能够实现不同的指标交由不同神经网络，通过每个网络的门控（分配不同权重）处理，减少指标间的干扰；且处理某个指标的神经网络规模减小，防止了过拟合。

模型结构：将用户、物品、统计、场景特征融合后，用若干个专家神经网络处理得到若干个向量，将这些向量依据权重加权平均得到最终的融合向量，进行后续处理，权重通过神经网络和softmax函数生成。专家神经网络数量一般为4或8。

**5、什么是极化现象？如何解决？**

极化现象：专家神经网络的权重一个接近1，其余接近0，导致没有将多个专家神经网络充分利用，没有被利用的专家神经网络处于“dead”状态。

解决方法：对softmax的输出使用dropout，强迫每个任务根据部分专家做预测，但结果可能偏差较大（特别是权重接近1的专家被dropout）。

**6、视频排序的依据？播放时长建模公式？**

依据：在多目标排序模型指标的基础上，还有播放时长和完播。

建模：用z代表经过全连接层输出的播放时长，p代表z经过sigmoid函数得到的值，p=exp(z)/1+exp(z)，用户真实播放时间为t，则y=t/1+t，对p和y用交叉熵函数使p接近于y。

**7、衡量视频完播的两种方法？完播率调整的原因？如何调整？**

两种方法：（1）回归方法，用实际播放时间/视频长度衡量播放率；（2）二元分类方法，定义完播指标（百分比），超过这个百分比代表完播，作为正样本，反之为负样本。全连接层输出的概率代表视频播放超过这个百分比的概率。

完播率调整：不能把预估的完播率用到融分公式中，否则有利于短视频，对长视频不公平；调整方式是将预估播放率/f(视频长度)作为融分公式的一项参与排序。

**8、物料画像由哪些组成？**

物料属性（即Item ID）、物料的类别和标签、基于内容的embedding、动态画像（限定时间下的点击数、点赞数等指标）、作者特征（发布笔记数、粉丝数、笔记消费指标等）、用户给物料反向打的标签。

**9、用户画像由哪两部分组成？**

用户的静态画像：人口统计学属性，如性别、年龄，以及其它稳定的数据信息，对推荐算法作用不大，用户唯一标识User ID是很重要的信息。

用户的动态画像：从用户历史交互行为提取出的用户兴趣爱好，如物料属性、用户动作类型（结合时间），可以采用简单平均、DIN、SIM等方法。

**10、物料画像和用户画像特征通常用什么方式来提取和查询？优缺点？**

方式：离线提取，在线查询。

优点：线上提取用户兴趣/物料特征只需要一个查询操作，耗时短，适合召回、粗排这种候选集庞大的任务。

缺点：提取出的用户兴趣不会随候选物料的不同而改变，针对性不强。

**11、偏差特征的定义？举例？如何处理这些特征？**

偏差特征：用户的选择并非完全出自他的兴趣爱好，而是受了场外不公平因素的影响，这些因素就是偏差。

例如位置偏差：用户对某笔记/视频没有点击，不是因为他不喜欢，而是笔记/视频出现的位置太偏不醒目，影响了用户的行动，造成了用户兴趣和反馈的不匹配。

处理方式：偏差特征通过一个线性层接入模型，保证无论这些特征取值如何，都不改变排序结果，实现偏差特征不起作用的效果。

**12、数值特征处理的方式？**

处理缺失值：用均值/中位数代替，或训练一个模型来预测缺失值。

标准化：将不同量纲，不同取值范围的数值特征压缩到同一数值范围内。对于长尾数据，常采用开方、取对数等非线性变换后再标准化。

数据平滑与消偏：威尔逊区间平滑（克服小样本的负面影响，提高计算置信水平）、CoEC（用某位置的点击数量均值代替，消除位置带来的偏差）。

分桶离散化：将连续特征值划分为多个区间，看数值落在哪个区间，就以那个桶的桶号作为特征值。

**13、类别特征有哪些？特点？**

用户画像、物料画像中的一二级分类、标签都是类别特征，特别关注User ID和Item ID。类别特征有高维、稀疏的特点。

**14、服务于类别特征的技术？**

（1）用embedding扩展特征内涵，多特征交叉；（2）利用Parameter Server的分布式集群并行处理；（3）利用FTRL（调节学习率）、DIN（调节正则系数）、FM（交叉特征其中一个不为0就能参与训练）等方法解决罕见特征受训不充分的问题。

**15、处理类别特征的数学方式？**

数学方式：映射（利用映射表映射到embedding矩阵，维护难度大）、特征哈希（根据整数大小映射到embedding矩阵，哈希冲突影响不大）。

**16、粗排和精排的区别？**

粗排是给几千篇笔记打分，单次推理代价很小，预估准确性不高，有截断；

精排是给几百篇笔记打分，单次推理代价很大，预估准确性更高，无截断。

**17、前期融合和后期融合的区别？分别适用什么场景？**

区别：前期融合是将所有特征串接一并输入神经网络（多目标排序模型），线上推理代价很大；后期融合是特征分别输入各自神经网络处理，得到各自处理结果最后再融合，线上推理代价很小。

场景：前期融合适用于精排，后期融合适用于召回。

**18、粗排使用的三塔模型结构？下层塔分别有什么大小关系？**

三塔模型介于前期融合和后期融合之间。下层分为用户塔、物品塔和交叉塔，分别对用户/场景特征、物品特征、统计/交叉特征进行处理，得到不同向量；上层将这些向量串接融合，分别输入不同指标的全连接层，最终输出指标结果，粗排推理绝大部分计算量在模型上层。

用户塔很大，一个用户只做一次推理；物品塔较大，利用缓存向量减少大部分推理；交叉塔较小，交叉特征是动态变化的，不能用缓存，必须每次做推理。

### 推荐系统公开课第四章：交叉结构+推荐算法实战第四章：精排（4.2）

**19、LR的基本原理？特点？**

LR是将输入和权重相乘，通过sigmoid函数输出。对类别特征而言，LR是非零输入对应的权重相加。

特点：LR强于记忆，将每个特征（主要是大众的，普遍的）的重要性（权重）牢牢记住，没有embedding和交叉作特征外延，无引申含义。

**20、FTRL原理的定性描述？损失之和如何定义？**

FTRL原理：为减少单个样本的随机抖动，第t步的最优参数不单单要最小化第t步的损失，而是让之前所有步的损失之和最小，在此基础上还增加正则项，即构成FTRL。

损失之和分为两项：第一项是前面t步的梯度向量之和与被前t条样本优化后的权重相乘；第二项是当前步被优化权重与历史权重之差的平方和，要求它们不能相差太远，以防损害对旧样本的拟合能力。

**21、FTRL是如何兼顾了预测精度和解的稀疏性的？**

预测精度：采用梯度累积的方式，防止单个样本的随机抖动，并为每个特征单独设置学习步长（特征出现频率与学习率成负相关）。

解的稀疏性：权重更新时设置了阈值λ，当梯度和权重计算的中间量z小于阈值时，权重会置零。

**22、FM的基本原理？空间上如何改进？**

FM对两个特征进行交叉，产生$w_{ij}x_ix_j$交叉项。由于n个特征交叉将引入n的平方个参数，因此将n×n矩阵拆分成两个n×k的矩阵相乘（其中一个为转置），k代表每个特征embedding的长度。于是，FM改进为 $(v_i^Tv_j)x_ix_j$。

**23、Wide&Deep的模型内容？**

Wide侧：处理普遍大众的特征，以及一些偏差特征（如位置偏差），利用LR计算。

Deep侧：将稀疏特征映射成稠密向量，通过DNN层得到输出结果。

Wide&Deep模型中，Deep是主力，Wide是辅助。

**24、DeepFM在Wide&Deep基础上做了什么改进？**

将Wide侧改为FM特征交叉，即模型由FM层和DNN层构成，FM和DNN的输入都是稠密的embedding向量。

**25、DCN交叉层的数学原理描述？如何做空间优化？**

原理：对向量$x_i$，将其进行Wx+b的操作后，与初始向量$x_0$按位相乘，并加上$x_i$，得交叉结果$x_{i+1}$。

空间优化：将n×n的W拆分成两个n×r的小矩阵。

**26、DCN-V2分为哪两个部分？两种不同的结构？**

分为交叉网络和DNN两个部分，两种结构为串联和并联。

**27、LHUC的基本原理？如何用于推荐系统的特征交叉？**

LHUC用于语音识别：将语音信号和说话者特征各自进行处理后相乘得到交叉向量，再将交叉向量和说话者特征相乘，以此不断交叉下去。

将语音信号、说话者的特征两个输入分别变成物品特征，用户特征，可用于推荐系统的特征交叉。

**28、SENet的模型内容？**

对m个特征进行embedding（长度可以不同），通过平均池化、非线性、全连接层等得到m个特征的权重，即每个特征的重要性，再将权重与对应embedding相乘得到新的向量作为结果，可以起到去噪的作用，可用于粗排和精排。

**29、双线性交叉的两种方式？**

内积：向量转置相乘；哈达玛乘积：向量对应位相乘。

**30、FiBiNet的模型结构？**

将SENet和双线性交叉进行结合，离散特征向量embedding以后分成三路，一路是直接串接，一路是双线性交叉，另一路是SENet+双线性交叉，三路向量串接融合，和连续特征一起输入上层神经网络进行后续处理。

### 推荐系统公开课第五章：用户行为序列建模+推荐算法实战第四章：精排（4.3）

**31、用户行为信息的构成有哪几个部分？**

每个视频ID的embedding组成的向量，观看该视频的时刻距离本次请求时刻的时间差，视频元信息（作者、来源、标签），动作程度（观看时长、完成度）等。

**32、简单平均的操作过程？适用模型？缺点？**

操作过程：对用户历史交互的LastN物品得到n个embedding向量，再对向量取平均表示用户特征。

适用模型：召回双塔模型、粗排三塔模型、精排模型。

缺点：简单平均提取出的兴趣是固定的，不会随候选物品的变化而变化。

**33、DIN模型的基本思想？内容？实现目标？适用模型？**

DIN利用注意力机制的思想，将用于精排的候选物品的向量作为查询，将用户历史交互的LastN物品作为键和值，依次计算候选物品与历史交互物品的相似度（查询-键的权重），再将相似度（权重）与对应LastN物品向量相乘，加权融合得到最终用户的行为特征。这种方法能实现用户的行为序列特征随候选物品的不同而改变，实现“千物千面”。

适用模型：DIN适用于精排模型，不适用于双塔、三塔模型，因为注意力机制要用到LastN+候选物品的融合，然而这两种模型用户塔和物品塔是解耦的。

**34、如何将双层attention用于DIN模型？**

在候选物品与历史交互物品attention之前，加一个自注意力机制模块，输入是用户历史交互的LastN物品原始向量，进入自注意力机制模块后输出的N个向量都融合了其它物料的信息，实现了特征交叉（类似autoint的原理），这里的self-attention可以用多头。

**35、DIN模型的缺点？SIM做了如何改进？**

缺点：建模的序列太短，容易有噪声；用户更早的交互行为可能对用户特征向量影响较大，而DIN将这类行为给忽略了，遗忘了用户的长期兴趣。

改进：SIM将用户历史交互的物品数量n变得很大（可以是几千），从这n个物品中选出k个相似物品作为注意力层的键/值输入，使得LastN变为TopK，实现了用户长期兴趣的关注和计算量的减少。

**36、选出k个相似物品的两种在线提取方式？维护的索引是否和主模型完全一致？**

硬搜索：根据候选物品的类目，在用户完整的长期历史序列中搜索与其有相同属性的历史物料。实际有数据库（如UBT）将用户长期行为序列缓存（用户ID-物料属性-物料属性对应的物品），所以只需在缓存中查找筛选。

软搜索：把候选物品向量作为query，做k近邻查找（也可用近似最近邻ANN），保留相似度最高的k个。实际是将物品的embedding存入向量数据库并建立索引，加速软搜索。

注意：离线索引中的embedding是另一套模型训练出来的，可能与推荐主模型不一致，且索引中的embedding更新频率不如主模型，所以会带来软搜索和SIM模型的差异。

**37、离线训练用户兴趣采用什么方式？简要原理？优缺点？**

采用双塔模型：一个塔用于提取用户长期兴趣，输入为用户A的画像和长期行为序列；另一个塔用于提取用户短期兴趣，输入为另一用户B的画像和短期行为序列。训练目标是鼓励同一用户的长短期向量相近（余弦相似度更大），不同用户的长短期向量较远（余弦相似度更小）。这种方式每天只需要更新一次即可，然后把用户兴趣向量存入数据库，预测时就拿用户的ID去数据库里检索得到用户兴趣向量。

优点：实现简单，耗时较少；缺点：得到的用户长期兴趣不会随候选物料变化。

### 推荐系统公开课第六章：重排

**38、如何基于物品属性标签度量相似度？**

由CV、NLP算法推断出类目、品牌、关键词等信息，再根据这些类目、关键词等设置的权重，加权计算总的相似度。

**39、基于物品向量表征衡量相似度中，学习物品向量有哪两种方式？各自特点？哪种更好？**

用召回的双塔模型学习物品向量，问题在于对新物品/长尾物品的学习效果不好。

根据CV、NLP模型提取出文字和图片的向量作为向量表征，思想是对于图片-文本二元组，预测图文是否匹配。优势是无需人工标注，且大部分笔记图文相关，这种方法优于双塔模型学习物品向量。

**40、重排在推荐系统链路中的位置？思想是什么？**

重排在精排给几百个物品打分之后，其思想在于保证被推荐物品的多样性。对于精排的几百个物品，既要考虑它们的打分，也要考虑物品之间的相似度，从而选出曝光的k个物品以及曝光顺序。

**41、简述MMR多样性算法的流程？**

设已选中物品的集合为S（一开始为空），未选中的物品集合为R；从R中选择精排分数最高的物品移到S，再计算集合R每个物品的MR分数（与它的精排分数和它与集合S中的物品的最大相似度有关，利用θ给两个因素分配权重，最大相似度一项起抑制作用），选择最大的MR作为MMR，对应物品从R移动到S；对上述操作进行k-1轮循环，结束后即可得到k个物品，每次MR和MMR都要重新计算。

**42、MMR计算的问题？如何解决？**

问题：当选中的物品越多（集合S越大），越不容易找出物品i，使得它与S中的物品都不相似，即最大相似度总是约等于1，导致MMR算法失效。

解决方法：引入滑动窗口，原本计算最大相似度的集合为S，现在改为滑动窗口对应集合W，表示最近选中的若干个物品。

**43、重排有哪些规则？如何将这些规则应用到MMR算法中？**

规则：（1）最多连续出现k篇同类型笔记（图文、视频）；（2）每k篇笔记最多出现1篇某种笔记（如每9篇笔记中最多只能出现1篇运营推广笔记）；（3）前t篇笔记最多出现k篇某种笔记（如前4篇笔记最多出现1篇带电商卡片的笔记）。

方式：重排结合MMR与规则，每轮先根据上述规则排除R中的部分物品，再对排除后的集合应用MMR公式。

**44、DPP的目标？利用其作为多样性衡量的依据？超平行体的体积和物品向量构成的行列的关系？**

目标：从集合中选出多样的物品，考虑选出物品集合的多样性。

依据：利用超平行体的体积衡量物品多样性，给定k个物品及其向量（单位化），如果这些向量两两正交，此时体积最大化，物品多样性好；如果向量之间线性相关，此时体积最小化，物品多样性差。

关系：超平行体的体积平方=det(物品向量构成的矩阵转置×它本身)。

**45、简述DPP多样性算法的流程？**

先精排给n个物品打分，从n个物品中选出k个物品组成集合S，保证精排分数之和越大越好，且k个向量组成的超平行体的体积越大越好。将选中的物品集合设为S，未选物品集合为R，每次从R中选出物品i加入S，在i的精排分数和加入S后超平行体的体积（即行列式一项）之间分配权重，每次要使求解公式值最大，进行k次循环即可选出k个物品作为重排结果。

**46、Hulu快速算法如何加快行列式的计算速度？**

可将行列式一项变成$A_{S∪{i}}$。它是在矩阵$A_S$基础上多一行和一列，计算该矩阵行列式时，可以利用Cholesky分解：有$A_S=LL^T$，其中L为下三角矩阵（对角线以上元素全为0）。每次多一行和一列时Cholesky分解情况变化不大，这样算行列式就会非常方便，减少运算。

**47、DPP如何结合滑动窗口和规则约束？**

当S集合中的物品越来越多时，行列式一项将趋近于0，导致DPP算法效果不好。于是将S变为滑动窗口集合W代表最近的若干个物品；以及在加入物品到S前，先从R中把不符合要求的物品排除，再从排除后的集合中选择物品加入S。