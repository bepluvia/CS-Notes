# 召回

**1、基于物料属性的召回过程？**

离线时按把相同属性的物料集合起来，构成属性-物料的map结构，线上提取用户喜欢的标签、关注的作者等信息，在map中检索返回相应物料。

**2、ItemCF的原理？如何计算用户对物品的兴趣评分？**

某用户喜欢物品1，物品2和物品1相似，那么该用户很可能喜欢物品2。

用户对某物品的兴趣=Σ用户对其它若干物品的评分×若干物品与该物品的相似度。

评分：点击、点赞、收藏、转发各为1分。

**3、如何计算两个物品的相似度？从矩阵的角度如何考虑？**

计算喜欢物品1的用户数量W1，喜欢物品2的用户数量W2，相似度=(W1∩W2)/sqrt.(W1·W2)。

若考虑喜欢程度，分子改为交集用户对物品1的评分×对物品2的评分再求和，分母W1项改为喜欢物品1的所有人喜欢程度的平方和再取模，W2项同理。

考虑用户反馈矩阵A（mxn），m是用户总数，n为物品总数，矩阵每个元素代表用户对物品的评分，计算 $S=A^TA$得到n×n矩阵，即可得到物品之间的相似度。为用户召回时，根据用户的历史交互，选出相似度最高的若干物品作为结果。

**4、ItemCF完整的召回流程？**

离线计算：建立用户→物品的索引，得到用户近期感兴趣的物品评分，以及物品→物品的索引，得到物品间的相似度。

线上召回：给定用户id，找到该用户近期感兴趣的last-n物品，每个物品找到top-k相似的物品，得到一共nk个相似物品，根据公式预估每个物品的兴趣分数，返回分数最高的若干物品作为结果。

**5、Swing模型的作用？利用Swing如何改写ItemCF相似度公式？**

Swing模型是为了降低小圈子的权重，防止物品的重合用户是来自同一小圈子对计算带来误差。

Swing要考虑重合用户集合V里两两之间喜欢的物品重合度。假设用户u1喜欢的物品为J1，用户u2喜欢的物品为J2，定义重合度overlap(u1,u2)=J1∩J2，重合度高，说明可能来自一个小圈子，要降低它们的权重。改写相似度公式= $Σ_{u_1∈V}Σ_{u_2∈V}1/α+overlap(u_1,u_2)$。

**6、ItemCF的优点？**

物料数量更少且特征更稳定，因此nxn的矩阵S不会很大，且可以提前计算好。以及S的计算有MapReduce分布式算法，计算方便。

**7、UserCF的原理？如何计算用户对物品的兴趣评分？**

用户1和用户2相似，用户2喜欢某物品，那么用户1很有可能喜欢这个物品。

用户对某物品的兴趣=Σ该用户和其它用户的相似度×其它用户对该物品的评分

**8、如何计算两个用户的相似度？公式如何优化？**

用户u1喜欢的物品为J1，用户u2喜欢的物品为J2，相似度=(J1∩J2)/sqrt.(J1·J2)。

公式的不足在于同等对待热门和冷门的物品，两个用户同时喜欢热门物品，不能完全说明两个用户相似，因此优化公式，将分子变为 $Σ_{l∈I}1/log(1+n_l)$。I是两个用户共同喜欢的物品，$n_l$代表喜欢物品l的用户数量。

**9、UserCF完整的召回流程？**

离线计算：建立用户→用户的索引，得到用户间的相似度，以及用户→物品的索引，得到用户对物品的兴趣评分。

线上召回：给定用户id，找到与该用户最相似的top-k用户；对这些用户，找到它们近期感兴趣的last-n物品，得到一共nk个相似物品，根据公式预估每个物品的兴趣分数，返回分数最高的若干物品作为结果。

**10、如何得到One-hot编码和Embedding向量？**

One-hot：把特征类别映射成序号，再把序号映射成高维稀疏向量，根据序号得到对应位置为1，其它全为0。

Embedding：把序号映射成低维稠密向量。Embedding参数以矩阵形式保存，大小为向量维度×类别数量。Embedding可由参数矩阵×One-hot向量得到。

**11、LR模型的内容？特点？问题？解决该问题的方式？**

内容：LR是一种评分卡，代表一些特征值构成的组合成交概率，分数越高越容易成交，LR最终得分是一条样本能够命中的评分卡中所有条目的得分总和。

特点：强于记忆，依赖人工输入，不能挖掘新模式，能通过正则剔除得分较低的罕见模式。

问题：弱于扩展，记住的是高频、大众的模式，缺乏挖掘小众需求，推荐个性化弱。

解决方式：将概念拆分成向量，实现将细粒度特征拆分成粗粒度的特征，从而使概念得到扩展，包含内容更加丰富。

**12、Embedding有哪两种类型？各自的内容？**

两种类型：共享Embedding和独占Embedding。

共享Embedding：能缓解由于特征稀疏，数据不足导致的训练不充分；复用Embedding能节省存储空间。例如：双塔模型中，候选物品和用户历史行为序列，以及特征交叉时会用到共享Embedding。但共享Embedding存在特征之间的互相干扰。

独占Embedding：避免特征之间的干扰，特征交叉时Embedding的独占（FFM/CAN）。

**13、FFM的内容？**

不同于FM的地方在于：FM中 $w_{ij}=v_iv_j$的v是在同一embedding中，而FFM的v是要在对方的embedding中找对应向量。它的embedding是独占的，但会引起参数爆炸。

**14、CAN的目标？模型内容？**

目标：让每个特征在与其他不同特征交叉时使用完全不同的embedding，同时不引入FFM那样那么多的参数导致空间爆炸和训练难度。

内容：将item的embedding拆解成若干个小矩阵，分别与user embedding通过MLP进行加权融合，从而实现特征交叉。也就是同一物品和不同用户交叉时，物品向量的不同位置发挥作用，相当于使用了不同的物品embedding，减少了不同交叉之间的相互干扰。

**15、矩阵补充模型结构？训练思路？**

结构：用户ID经过embedding层得到用户向量，物品ID经过embedding层得到物品向量（两个embedding不共享参数），两个向量进行内积，得到用户对物品的兴趣。

训练：有用户embedding和物品embedding两个参数矩阵A和B，第u号用户对应矩阵第u列，第i号物品对应矩阵第i列。将两个矩阵相乘，最小化平方和损失，即真实兴趣分数y与两个向量内积之差。

两个矩阵相乘的结果里有很多空白值，我们的目标是通过学习矩阵A和B，来预测空白位置的值，补充结果矩阵的内容，补全后就可将分数较高的物品推荐给用户。

**16、矩阵补充的缺点？**

（1）只用了用户和物品的ID embedding，没利用物品和用户的属性，改进方法是采用双塔模型。

（2）负样本的选择方式不对：不能用曝光未点击的方式作为负样本。

（3）内积不如余弦相似度反映用户兴趣，平方和损失不如交叉熵损失效果好。

**17、如何利用矩阵补充做线上召回？会出现什么问题？如何改进？**

模型存储：训练得到用户矩阵和物品矩阵A和B，A的每一列代表一个用户，B的每一列代表一个物品。矩阵A存储在key-value表，B的存储比较复杂。

线上服务：采用最近邻查找（KNN）——给定用户ID得到他的向量a，查找他最感兴趣的k个物品作为召回结果。这种方法需要对B矩阵的每一列进行内积，计算量巨大。

改进方式：近似最近邻查找（ANN），采用聚类思想。对物品数据进行预处理，将相似的物品聚在一起作为同一扇形区域，每个扇形区域用一个单位向量表示。每个扇形区域向量作为key，区域内的点（物品）作为value。于是用户a与物品的内积转变为与扇形单位向量内积，得到相似度最大的向量，返回向量内的所有物品。

**18、向量化召回的4个维度？**

如何定义正样本，如何定义负样本，如何将用户、物品特征转为embedding，如何定义损失函数。

**19、如何定义正样本？存在问题？解决方案？**

正样本的定义：

对用户和物品而言，曝光且有交互（如点击、点赞、购买等）的用户-物品二元组构成正样本；

对物品和物品而言，在同一会话（间隔时间较短的用户行为序列）交互过的两个物品构成正样本；

对用户和用户而言，两个用户的交互历史相近，构成一堆正样本。

存在问题：少部分物品占据了大部分点击，导致正样本大多是热门物品，导致热门物品更热，冷门物品更冷。

解决方案：过采样冷门物品，降采样热门物品。

**20、如何定义负样本（两种类型）？两种采样方式及具体内容？**

简单负样本：未被召回的物品。

复杂负样本：与正样本有几分相似，但细节有所不同，或者说用户对这些样本多少有点兴趣，但兴趣不够（召回但在排序阶段被淘汰的物品）。

采样方式：随机采样或batch内负样本。

（1）随机采样：分为均匀采样和非均匀采样，均匀采样对冷门物品不公平，因此要用非均匀采样过采样热门物品，抽样概率正比于点击次数的0.75次方。

（2）batch内负样本：每个用户对应点击的物品构成用户-物品正样本，该用户与其它用户点击的物品构成负样本（都是简单负样本）。batch内负样本的问题在于热门物品成为负样本的概率过大，过采样程度过大，缺乏冷门样本（easy negative）。因此需要引入 $-logp_i$纠偏，将预估值抬高，注意纠偏只在训练中起作用。

**21、能否将曝光未点击的物品作为负样本？为什么？**

不能。召回的目标是将用户可能喜欢的和完全不感兴趣的物品区分开，不是优中选优。曝光未点击的物品实际上用户是有兴趣的，只是兴趣不够。将这部分物品作为召回负样本，违背了召回的目的，无法让模型见世面（既看到最匹配的组合，也看到最不靠谱的组合），它们应当在排序阶段被区分。

**22、召回要求解耦指的是什么？原因？具体方式？**

解耦：将用户信息和物料信息隔离，先各自处理，最后再将两个embedding交叉得到结果。

原因：召回的样本数量很大，如果每个用户都进行DNN那样的复杂运算，会使推荐系统负担过重，效率过低，不能满足在线实时性的要求。

方式：先将物品向量离线计算好，存储并建立好索引。在线时利用近似最近邻快速搜索与用户最相似的物品向量。

**23、召回损失函数有哪些？**

NCE Loss：基于交叉熵损失函数，与正样本的内积和负样本的内积（随机采样一部分物料）相关，同时引入纠偏项。

NEG Loss：NCE的简化，忽略纠偏项。

Sampled Softmax Loss：基于softmax函数，分子反映正样本的相似度值，分母包含正样本和负样本的相似度值，同时引入纠偏。

Pairwise Loss：鼓励正样本的余弦相似度大于负样本的余弦相似度，分为triplet hinge loss（MH Loss，直接考虑内积）和triplet logistic loss（BPR Loss，将内积作为指数）。

**24、借助Word2Vec的召回方式有哪些？Word2Vec分为哪两类？比较异同？**

召回方式：Item2Vec、Airbnb、EGES召回。

Word2Vec：Skip-Gram和CBOW。前者是用中心词预测上下文，后者是用上下文预测中心词。采用NEG Loss进行训练，正样本是中心词上下文的目标词汇，负样本是为中心词随机采样的一批单词。

比较：Skip-Gram预测次数更多，对大规模数据集，生僻词/专业术语较多的情况更适合。

**25、Item2Vec的原理？如何利用4个维度进行召回？Item2Vec存在的问题？**

原理：将用户某一行为序列（一个会话内点击过的物料）当成一个句子，序列中的每个ID当成一个单词。套用Word2Vec训练就能得到每个物料的embedding，用于I2I召回。

召回：

（1）如何定义正样本：利用滑窗技术，对同一用户同一会话交互过的物料，某个物料前后出现的其他物料认为彼此相似，成为正样本。

（2）如何定义负样本：从整个物料库中随机采样一部分物料作为负样本。

（3）如何embedding：定义待学习的矩阵V，装载物料的embedding。

（4）如何定义损失函数：采用NEG Loss。

问题：滑窗导致Item2Vec缺乏对长时间序列的相关性探究，但在推荐系统里面，点击的第一个物料和最后一个物料存在高度相关的可能。

**26、Airbnb召回在Item2Vec基础上做了哪些改进？**

Airbnb以民宿房间为背景，对I2I召回：

（1）正样本：点击序列的每个房屋和最终成功预订的房屋是相似的。

（2）负样本：引入hard negative，将每个房屋与其它同城的房屋作为负样本。

（3）损失函数：同样采用NEG Loss，但增加了额外的正负样本。

对U2I召回（拓展至冷启动领域）：

（1）正样本：某用户u预订过某房间l，u属于的类别U和l属于的类别L的向量相似，成为正样本。

（2）负样本：对一个用户类别U，随机选择一部分房屋作为easy negative；同时引入hard negative，将被房东拒绝的用户类别U与该房屋类别L作为负样本。

（3）如何embedding：考虑了第i类用户和第i类房屋的embedding，利用人工先验规则进行分类。

（4）损失函数：同样采用NEG Loss，但增加了额外的负样本。

**27、EGES是针对什么背景的召回？如何利用4个维度进行召回？**

EGES相比Airbnb I2I，Item2Vec破除了同一用户同一会话的限制，考察了跨用户，跨会话的情况。 

EGES召回：

（1）正样本：根据用户行为序列构建物料关系图，每一条边代表两个物料被顺序交互过，根据关系图随机游走生成若干新的序列，对这些新序列利用Word2Vec定义滑窗，窗口内的两个物料认为相似，构成正样本。

（2）负样本：采用随机采样的方式。

（3）如何embedding：将物品ID的embedding和属性的embedding合成新物料的embedding，合并方法有简单平均和加权平均，融合过程是由算法自动学习。

（4）损失函数：采用NEG Loss。

**28、FM如何运用于召回中？**

对给定用户，考虑 $F(u,t)=W_t+V_{tt}+V_{ut}$，第一项是所有物品的权重，第二项是物料特征内部的两两交叉，第三项是每个用户特征和每个物料特征的两两交叉（各自要预先得到），采用BPR Loss进行评估。预测时将物品特征和用户特征拆开（$E_u×E_t$）。

**29、双塔模型的内容？如何负采样？混合负采样的原理？**

分为用户塔和物品塔：用户塔将用户ID、用户离散特征、用户连续特征通过特征变换，再进入神经网络学习得到用户表征向量，物品塔同理得到物品表征向量，将两个向量求得余弦相似度作为用户对物品的兴趣评估。

负采样方式：batch内负采样，混合负采样。

混合负采样：额外建立一个向量缓存，存储训练中得到的最新物料向量，除了batch内负采样得到的hard negative，再从向量缓存中采样一些向量作为easy embedding。

**30、双塔模型的特点？生成用户向量过程中如何将历史行为赋予不同权重？**

特点：双塔模型要求严格解耦，单塔可以很复杂，求余弦相似度前不允许用户和物品特征进行交叉。

如果想将用户不同历史行为赋予权重作为用户向量，将物料作为query进行attention行不通，可以做出如下替代方案：

（1）用户输入的搜索文本最能反映用户当下意图，作为query；（2）将用户画像作为query给历史行为打分；（3）将用户最后点击的物料作为query衡量历史行为重要性。

**31、双塔模型的训练方式？**

训练方式：

pointwise：独立看待每个正样本和负样本，做简单二元分类；

pairwise：对给定用户，每次取一个正样本和一个负样本，鼓励用户与正样本相似度越大，与负样本相似度越小，采用Pairwise Loss；

listwise：对给定用户，取一个正样本和多个负样本，鼓励正样本余弦相似度尽量大，负样本余弦相似度均尽量小，采用Sampled Softmax Loss。

**32、Sampled Softmax Loss的训练技巧？**

（1）L2正则化：用户和物品的向量点积采用余弦相似度，体现出归一的过程；

（2）引入温度调整系数：用户和物品的相似度乘上1/τ，用于放大没被训练好的负样本，损失增加，该样本被重点关注。

τ如果太小，会导致推荐精度高，但对用户潜在兴趣探索不够（用户没点击某物品并不完全因为不感兴趣，还可能是还未被曝光），导致进入信息茧房。

τ如果太大，召回把关太松，影响用户体验，但有利于实现推荐多样性。

（3）采样概率修正：负样本包括batch内负样本和向量缓存的负样本，纠偏的概率变成两种负样本的组合概率。

**31、双塔模型的线上召回过程？为什么要分为离线存储和在线存储？**

物品塔：离线存储，将训练好的物品向量存入向量数据库，并建立向量-ID索引，加速最近邻查找。

用户塔：给定用户ID和特征线上计算用户向量，将其作为查询在数据库中做最近邻查找，返回与用户向量余弦相似度最高的物品向量。

原因：线上计算几亿物品向量代价过大；且用户兴趣是动态变化的，必须线上计算，而物品特征相对稳定，因此可以线上计算物品向量。

**32、双塔模型的两种更新方式？**

全量更新：今天凌晨用昨天全天的数据，训练1 epoch，即每个数据只用一次（要randomshuffle打乱），目的是消除偏差。

增量更新：做online learning更新模型参数，实时捕捉用户变化的兴趣，按数据从早到晚的顺序训练1 epoch。增量更新对数据流要求高，同时对时间延迟有严格要求。

**33、双塔模型的问题？如何进行改进？改进方式的目标是什么？**

双塔模型对高点击物品学习得好，长尾物品（低曝光物品）学习不好。可采用自监督学习方法，做data augmentation，把长尾物品向量表征学得更好，把低曝光物品、新物品推荐更准。

**34、特征变换的方式有哪些？**

random mask：随机选一些离散特征，把它们遮住；

dropout：随即丢弃特征中的50%值；

互补特征：把特征随机分成两组输出两个物品表征；

关联特征：随机选择一个特征作为种子，利用互信息大小将相关性大（互信息大）一组关联特征mask，保留剩下的特征。

**35、自监督训练的过程？**

将物品i做数据增强，得到特征i1和i2，物品j同理得到特征j1和j2，目标是同一物品i1和i2的向量（正样本）相似度大，不同物品之间i1和j1/j2向量（负样本）的相似度小，采用交叉熵损失函数衡量。

**36、Deep Retrieval的内容？四要素？**

内容：把物品表征为路径，先上查找用户最匹配的路径。

（双塔模型以向量作为中介，Deep Retrieval以路径作为中介）

四要素：索引，神经网络预估路径，线上召回，训练。

**37、Deep Retrieval的索引分为哪两部分？**

（1）物品→路径：一个物品对应多条路径。

（2）路径→物品：一条路径对应多个物品。

**38、Deep Retrieval如何利用神经网络预估路径？**

用3个节点[a, b, c]表示一条路径，a代表给定用户特征x，预估用户对节点a的兴趣 $p_1(a|x)$；b代表给定x和a，预估用户对节点b的兴趣 $p_2(b|a;x)$；c代表给定x、a和b，预估用户对节点c的兴趣 $p_3(c|a,b;x)$，将3项乘起来则代表用户对该路径的兴趣 $p(a,b,c;x)$。

**39、Deep Retrieval如何进行线上召回？beam search的内容？如何选择beam size？**

召回过程：给定用户特征，用神经网络做预估，用beam search召回一批路径；然后利用索引路径→物品召回一批物品；再对物品打分排序，选择一个子集。

beam search：设置参数k作为beam size，代表从一个节点出发选择k条最优路径。它本质是一种贪心算法，分别让p1，p2，p3最大化，但单独最大化这3项得到的结果不一定是最优路径。k越大，越容易选出最优路径。

**40、Deep Retrieval训练步骤？正则项的作用？**

训练步骤（以下两个步骤是同时进行的）：

（1）学习神经网络参数（用户到路径）：要让物品表征的路径兴趣分数更大，损失函数则为兴趣分数取对数再取相反数。

（2）学习物品表征（以用户为中介，探究物品和路径的关联）：用户→物品看的是用户是否点击过物品，用户→路径则为（1）中内容，两者相乘即为物品和路径的相关性，损失函数方法同（1）。

正则项：在学习物品表征时，避免过多item集中在一条路径上，在损失函数中加上正则项，做到路径上item数量的平衡。

**41、说出其它的一些召回通道？**

地理位置召回：GeoHash召回（经纬度编码）和同城召回。

作者召回：关注的作者，有交互的作者召回，相似作者召回。

缓存召回：精排没有被曝光的缓存起来，作为一条召回通道，缓存要实行退场机制（成功曝光、超出缓存大小、超过召回次数、达到保存最多日期）

**42、曝光过滤问题指的是什么？采用什么方式？**

曝光过滤问题：如果用户看过某个物品，则不再把该物品曝光给该用户，曝光过的物品会从召回结果中被排除，采用Bloom Filter的方式。

**43、Bloom Filter的作用？有什么问题？**

作用：判断一个物品ID是否在已曝光的集合中，如果判断为no，则一定不在集合中（一定没被曝光）；如果判断为yes，则很可能在集合中（有可能被曝光，有可能没被曝光），将被舍弃。

问题：这种方式一定可以避免物品重复曝光，但容易误伤，把未曝光物品视为已曝光物品过滤掉。

**44、Bloom Filter具体做法？如何定性分析误伤概率？**

具体做法：把每个物品映射到0到m-1的向量空间内，设置哈希函数k代表映射的空间位数。每往集合添1个曝光物品，只需把向量k个位置设为1，原本为1则不变。k＞1时，必须映射的每一位都为1才会被判断为已曝光。

误伤概率：设曝光物品集合大小为n，二进制向量维度为m，使用k个哈希函数。如果n越大，向量中的1越大，误伤概率越大；m越大，向量越长，越不容易发生哈希冲突，误伤概率越小；k太大或太小都不好。

（由于年龄大于1个月的物品不可能被召回，因此没必要把它们记录在Bloom Filter中，将它们移除可以减少n，降低误伤率）

**45、Bloom Filter曝光过滤的完整链路？**

前端记录下所有被曝光的物品，写入Kafka队列；用Flink对队列内容实时读取，并实时将结果写入向量空间；再将得到的向量空间应用于召回物品中，过滤掉判断为已曝光的物品，剩下的进入排序和曝光环节。

**46、Bloom Filter曝光过滤的缺点？**

只支持添加物品，不支持删除物品，从集合中移除物品，无法消除它对向量的影响（不能简单地把对应位置从1改为0）。

**47、GCN的内容？特点？如何定义正样本、负样本及损失函数？**

内容：将推荐系统构建成一张图，各种实体（用户、商品、店铺、品牌等）构成图的顶点，各种互动关系（浏览、点击、购买等）构成图的边。GCN将召回建模为边预测问题，预测两节点之间是否有边存在。

特点：GCN考虑了图中的拓扑连接关系，有传递信息的作用。它能让两个用户的信息，通过共同购买过的商品、共同关注的品牌和商店进行相互传递；也能让两个物料的信息，通过同属的标签、品牌等相互传递，从而丰富用户和物料建模时的信息来源。

定义：一条边上两端节点v1和v2构成正样本，v1+随即采集的顶点构成负样本，损失函数利用NEG、SS、MH、BPR等都可以。

**48、GraphSAGE的原理？和GCN相比有什么优势？**

GraphSAGE是GCN的一种实现思想。它并不直接学习图上各节点的向量，而是学习出一个转换函数，只要输入节点的特征和它的连接关系，它就能返回该节点的向量表示。

优势：GCN需要将整个图的邻接矩阵输入模型，这意味着它在处理大规模图时会遇到内存和计算资源的瓶颈，遇到未见过的新节点时存在预测局限；而GraphSAGE是采样固定数量邻居节点进行聚合，避免了GCN需要整个图的限制，能很好的处理未见过的新节点，提高模型拓展性。

**49、GCN的具体实现过程（定性，以推荐算法实战p133为例）？GAT如何改进GCN？**

总结：自上而下生成节点，自下而上逐层卷积。

生成节点：考虑节点A（最高层），先生成和它直接相邻的节点B、C、D（次高层），再对B、C、D生成它们各自相邻的节点（底层）。

逐层卷积：从底层开始聚合邻居节点的信息（AGG聚合+PROJW1权重分配），并和本节点信息（PROJB1权重分配）合成，逐层操作得到最高层节点A的信息。聚合邻居节点信息时可以考虑拼接、attention等方式。

GCN的聚合权重是固定的，而GAT通过attention生成权重，让模型自己学习，使处理更加灵活。

**50、GCN召回的实例？目标是什么？同构图的含义？**

实例：PinSage，图中有Pin和Board两种节点，分别代表网址和收藏夹，两节点的连接边代表收藏这一动作，它的目标是学出高质量的Pin Embedding。

同构图：一类节点和一类边的关系图。

**51、PinSage的训练和推理技巧？**

训练：（1）Mini-Batch训练，将关联的节点有限化，减少计算时间和内存，提高效率；（2）选取邻居关联节点的方式是随机游走，选取被访问次数最多的若干个节点，剩余的节点可以作为Hard negative样本（相关但是相关性不够）。

推理：MapReduce分布式推理，map是指各节点独立将上一轮卷积得到的旧向量映射成新向量，并发送给邻居节点；reduce是指各节点聚合邻居节点发来的向量，再映射成新向量。

**52、异构图的GCN两种方式是什么？**

MuitiBiSage：多个同构图卷积，思路是先考虑单一关系的同构图的节点信息传递，然后再将每种单一关系得到的结果融合（用到self-attention）起来生成节点向量。

GraphTR：直接在异构图上进行卷积，先用transformer实现同一类型的邻居节点内部信息交叉，再用FM实现不同类型邻居节点之间的信息交叉。