损失如何使用？

基于深度学习的新闻推荐系统应用
# 数据层面

## 1. 数据正负样本比例是多少，进行负采样了吗，为什么不在召回时就筛除负样本，而是在召回后进行？

**（1）数据正负样本比例是多少，进行负采样了吗？**

在训练新闻推荐系统时，正负样本比例大约控制在 **1:4 或 1:5**，即每个正样本（用户点击）对应采样4到5个负样本（用户未点击但曝光过的新闻）。

在58同城竞赛中，Click（CTR）数据正负比例大概1:10左右；send（CVR）数据正负比例大概1:12左右；

> 为什么在排序模型中做**负采样**？
>
> 第一，正负样本天然极度不平衡，用户点击的内容只占极少数，不采样的话排序模型容易学习到“全部不点”的策略；
>
> 第二，从计算效率考虑，全量负样本量过大，不可能全部训练。

**（2）为何不在召回阶段就筛除负样本，而要放到召回后再处理？**

主要是因为召回阶段的目标和排序阶段不同：

召回阶段是**高召回率优先**，宁可多一些冗余结果，也不能漏掉潜在的好内容。

- 比如协同过滤、向量相似、召回粗打分等，只是快速生成一个候选池；
- 若召回阶段筛除“非点击”就等于用训练标签提前干预了召回的探索空间，可能漏掉新内容或冷启动内容；

而 负样本采样适合在召回后的排序阶段 进行，因为这时我们**已经有一个相对可控的小候选池，能更准确构造负样本**（比如根据曝光未点击、点击时延等策略来采样）。

总之，召回阶段追求“全”+快，排序阶段才追求“精”+准，所以负样本采样在排序前进行更合理。



## 2. 训练集负采样后测试集需要处理吗？是怎么处理的？

**（1）训练集负采样后测试集需要处理吗？**

训练集如果用负采样，测试集**无需**负采样，原因在于：

- 测试集负采样就无法反应真实业务场景，而且模型部署时系统也是面临

- 会显著影响评估指标：

  > 排序模型的本质任务是**从一堆候选中把最相关的排前面**；
  >
  > 如果你人为删掉一堆负样本（未点击的），就丢失了原始的排序难度；
  >
  > 举个例子：原本系统展示了100篇文章，用户点了第3个；你删掉了90篇负样本，只留10篇，那评估的Recall@5等指标将明显偏高，但并不反映模型真实效果。

**（2）如何解决负采样后分布改变的问题？**

视情况而定，如果训练时进行负采样，但是测试效果不好，可能会出现下列问题：

1. 模型学到的是“采样分布”下的点击概率，不是真实分布下的CTR；

> 举个例子，假设训练时每个正样本配5个负样本，模型学到的是“在1:5分布下，点击的概率”，这跟线上全量曝光下真实 CTR 有偏差；

2. **模型对 Hard Negative 学得特别好，但对 Easy Negative 没概念；**

> 导致模型排序边界在测试时不稳定，线上表现与离线差异较大。

理论上训练集如果有负采样，训练策略要有所改变：

✅ 方法一：训练时进行 偏差矫正（重点）

1. **加权训练（Sample Weighting / Importance Weighting）**

> 给训练集中每个样本分配一个权重，反映它在真实分布下出现的概率。

- 方案如：
  - 曝光频率低的 item 给更高权重；
  - 使用点击率预估模型估算真实曝光概率作为反向权重（Inverse Propensity Score，IPS）；

2. **使用 Exposure-aware Loss（例如 BPR with Exposure）**

> 不仅考虑点击，还考虑曝光概率，建模用户看到的概率，例如：

- Loss = p(expose) × p(click | expose)
- 典型论文：Deep Interest Evolution Network（DIEN）中也有建模曝光概率模块。



## 3. 负采样和特征工程有什么关系？负采样在最终的指标上是否带来了提升？

**（1）负采样和特征工程有什么关系**

负采样控制的是**样本构成和数据分布**，决定哪些样本参与训练；

特征工程影响的则是输入数据的**表征方式**，让模型更容易学习出判别边界。

两者虽是不同模块，但存在一定耦合：**如果负采样策略不合理，会影响特征的重要性分布**，从而干扰模型的特征权重学习。例如：

- 如果负样本太简单，模型可能只学习到了“容易区分”的特征，而无法对“高质量”负样本做出有效判断；
- 如果引入了**Hard Negative（近似点击但未点击）**，我们就更依赖于精细的特征设计去区分这些“伪正样本”，从而倒逼我们优化特征，如引入更精细的用户行为序列、内容相似度等特征；

**（2）负采样在最终的指标上是否带来了提升**

> 在58同城项目中并没有对负样本进行采样，因为负样本比例也不算太大，后续实验可以跟进一下，再完善结果。

在我的“新闻推荐应用”项目中，我尝试从原来的随机负采样切换为：

- 曝光但未点击（近似真实场景）；❌
- 再进一步引入 Hard Negative（高粗排得分但未点）；✅

> 但是我是同步新闻推荐竞赛数据集作为训练数据的，原始数据只有点击数据，因此无法获取到曝光数据，因此只实现了Hard Negative方法。

**实际提升：**

- AUC 提升了约 **1.5~2%**；
- 实际线上 CTR 提升也达到了可观的 **0.3~0.5%**。

原因在于：

- 更真实的负样本分布（曝光未点）能更好逼近线上预测场景；❌
- 加入 Hard Negative 增强了模型的判别能力，让模型更关注“容易混淆”的样本。✅

所以说，**负采样不仅提升模型泛化能力，也提升了模型学习重点特征的能力**，最终对指标带来了实际收益。











# 特征工程





# 模型







## 58同城AI算法大赛

### 1. DIN怎么做的？最后怎么pooling的，是average pooling还是sum pooling？

- DIN考虑到用户的历史行为商品与当前商品广告的一个关联性，利用计算候选商品和历史商品之间的相关性计算出注意力机制权重，表示用户历史行为的各个商品的重要程度大小；
- DIN主要通过局部注意力单元实现，局部注意力单元的输入为用户历史行为商品、当前的候选商品和两者的外积，concat之后送入激活函数，然后通过一个线性层得到对应的注意力分数。最后将权重与原来的历史行为embedding相乘，再pooling就得到了用户的兴趣表示；
- DIN采用的是sum pooling，论文表示sum pooling效果最佳；



### 2. 介绍一下DIN和其他推荐系统常用的排序模型

#### 2.1 DIN

> 核心思想：对用户行为序列建模时，引入了“局部激活机制”，让用户历史兴趣对当前目标 item 自适应关注。

**模型结构**

1. 候选item-feature与用户行为序列中item的features，依次计算相似度（Activation Unit）。

> **步骤 1：拼接行为与目标，构造匹配向量**
>
> 对每一个历史行为 $h_i$，和目标 item $e_{\text{target}}$ 做拼接、差值、乘法：
> $$
> x_i = [h_i, e_{\text{target}}, h_i - e_{\text{target}}, h_i \odot e_{\text{target}}]
> $$
>
> 这里 $[]$ 表示拼接，$\odot$ 是逐元素乘法，最终形成一个 序列长度 x $[h_i, e_{\text{target}}, h_i - e_{\text{target}}, h_i \odot e_{\text{target}}]$。
>
> **步骤 2：送入小型 MLP（MLP for attention）**
> $$
> a_i = \text{MLP}(x_i) \in \mathbb{R}
> $$
> 即每个行为 token 得到一个 attention 分数 $ a_i $。
>
> **步骤3：Softmax 归一化权重**
> $$
> \alpha_i = \frac{\exp(a_i)}{\sum_{j=1}^{T} \exp(a_j)}
> $$
>
> 得到每个历史行为的 attention 权重 $ \alpha_i $。

2. 将相似度与序列item的features加权求和（SUM Pooling），得到用户的行为序列兴趣：
$$
v_{\text{user}} = \sum_{i=1}^{T} \alpha_i \cdot h_i
$$
​	 $ v_{\text{user}} $ 就是被目标 item 激活后的用户兴趣表达，会和目标 item、用户静态特征等一起送入 DNN 排序塔中。

3. 把user features、刚刚得到的行为序列features、候选item-feature和其他交叉特征等concat起来，送入一个DNN网络，最终预测得到user对候选item的CTR评分，然后经过softmax网络映射到0～1（表示概率）；

#### 2.2 与 DIN 相关/常用排序模型对比

| 模型名                        | 简介                          | 特点                              | 是否建模序列 | 是否目标相关 |
| ----------------------------- | ----------------------------- | --------------------------------- | ------------ | ------------ |
| **LR（Logistic Regression）** | 线性模型                      | 简单、可解释，适合冷启动          | ❌            | ❌            |
| **Wide & Deep**               | LR + DNN                      | 结合记忆（Wide）和泛化（Deep）    | ❌            | ❌            |
| **DeepFM**                    | FM + DNN                      | 自动学习交叉特征，无需人工组合    | ❌            | ❌            |
| **DIN**                       | Deep Interest Network         | 目标相关兴趣建模，使用 attention  | ✅            | ✅            |
| **DIEN**                      | DIN 的升级版                  | 加入 GRU 建模兴趣演化（时间依赖） | ✅            | ✅            |
| **DSIN**                      | Deep Session Interest Network | 多兴趣 + session 粒度建模         | ✅            | ✅            |
| **MMOE / PLE**                | 多任务模型                    | 多目标（如点击 + 收藏）共享表达   | ❌            | ❌            |
| **Transformer / BERT4Rec**    | 基于自注意力                  | 强序列建模能力，参数多            | ✅            | ✅            |
| **DSSM / YouTube DNN**        | 用户和 item 用双塔建模        | 计算效率高，可用于召回也可排序    | ✅（弱）      | ❌（一般）    |

> 目标相关*：就是指面对不同目标，对应不同兴趣向量；（比如在DIN中利用attention，但是DeepFM等只是简单sum pooling）

### 3. 尝试过DIN的变体吗？有哪些改进？指标有提升吗？

#### 3.1 DIEN、DSIN、SIM 分别针对哪些问题优化？



### 4. DIEN、DSIN、SIM针对哪些方面做了优化？







# 训练

## 1. 有没有出现过拟合？怎么判断的？你是怎么处理的？

**有没有出现过拟合？怎么判断的？**

出现过，58同城竞赛初始阶段，只使用简单的feature做测试时（没有用到交互序列和用户label特征），出现了过拟合现象。主要体现在**训练集 vs 验证集指标差距明显**：

- 比如训练集 AUC 达到 0.7+，但验证集上升到0.6+的时候，就开始下降；
- Logloss 在训练集持续下降，但验证集在一定轮次后上升；

**怎么处理的？**

（1）Early Stopping

- 设置验证集观察窗口 `patience=3~5`，在验证集指标不再提升时自动停止；
- 这样能有效防止后期过拟合训练集；

（2）调整模型结构

- 根据训练难度，调整expert数量，还有DNN层数和宽度；

> 最终expert确定为4，embedding确定为32；

- 将原来的 5 层 DNN 简化为 3 层，宽度也做了适当收缩；
- 在 MMOE 模型中，减少了 expert 数量，防止过拟合多任务样本偏差；

（3）增强数据多样性

- 对负样本引入了 **Hard Negative 采样**，提升训练样本的“边界难度”；
- 扩展用户行为序列长度，增加用户长期兴趣信息，增强泛化；

（4）多任务模型

多任务结构本身也能一定程度上缓解过拟合，比如通过共享底层表示，让稀疏任务受益于主任务的泛化能力。

**结果如何？**

最终验证集 AUC 提升了约 **2%**，logloss 降低约 **0.01**，
并且在小规模线上实验中 CTR 提升了 **0.3%+**，稳定性更好。
总体来说，过拟合问题得到明显缓解，模型的泛化能力显著增强。



## 2. Embedding维度是如何确定的？

组要看类别数量，通常取 $int(log_2(类别数))$ 或者 $int(类别墅)^{\frac{1}{4}}$：

```
embedding_dim = int(log2(num_unique_categories))
embedding_dim = int(num_unique_categories ** 0.25)
```

或直接分组设置，例如：

| 类别数量范围 | 建议维度 |
| ------------ | -------- |
| < 10         | 4–8      |
| 10–100       | 8–16     |
| 1k–10k       | 16–64    |
| 10k–1M       | 64–128+  |

因为项目中的类别数几乎都小于1k，因此取emb_dim = 32应该比较合理。

